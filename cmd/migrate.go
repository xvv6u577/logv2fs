/*
Copyright Â© 2023 NAME HERE <EMAIL ADDRESS>
*/
package cmd

import (
	"context"
	"fmt"
	"log"
	"strings"
	"time"

	"github.com/google/uuid"
	"github.com/spf13/cobra"
	"github.com/xvv6u577/logv2fs/database"
	"github.com/xvv6u577/logv2fs/model"
	"go.mongodb.org/mongo-driver/bson"
	"gorm.io/gorm"
)

// migrateCmd represents the migrate command
var migrateCmd = &cobra.Command{
	Use:   "migrate",
	Short: "å°†MongoDBæ•°æ®è¿ç§»åˆ°PostgreSQLæ•°æ®åº“ (æ··åˆè®¾è®¡æ¨¡å¼)",
	Long: `è¿™ä¸ªå‘½ä»¤å°†æ‰§è¡Œä»MongoDBåˆ°PostgreSQLçš„å®Œæ•´æ•°æ®è¿ç§»ã€‚
	
æ··åˆè®¾è®¡ç­–ç•¥:
- æ ¸å¿ƒå­—æ®µä½¿ç”¨å…³ç³»å‹è®¾è®¡ï¼Œä¾¿äºæŸ¥è¯¢å’Œç»´æŠ¤
- æ—¶é—´åºåˆ—æ•°æ®ä½¿ç”¨JSONBå­˜å‚¨ï¼Œä¿æŒçµæ´»æ€§
- æ”¯æŒå¢é‡è¿ç§»å’Œæ–­ç‚¹ç»­ä¼ 
	
æ”¯æŒçš„è¿ç§»ç±»å‹:
- schema: ä»…åˆ›å»ºPostgreSQLè¡¨ç»“æ„
- data: ä»…è¿ç§»æ•°æ® (éœ€è¦å…ˆåˆ›å»ºè¡¨ç»“æ„)
- full: å®Œæ•´è¿ç§» (è¡¨ç»“æ„+æ•°æ®ï¼Œé»˜è®¤é€‰é¡¹)

ä½¿ç”¨ç¤ºä¾‹:
  # å®Œæ•´è¿ç§» (æ¨è)
  ./logv2fs migrate --type=full

  # ä»…åˆ›å»ºè¡¨ç»“æ„
  ./logv2fs migrate --type=schema

  # ä»…è¿ç§»æ•°æ®ï¼Œæ‰¹é‡å¤§å°500ï¼Œè·³è¿‡é‡å¤è®°å½•
  ./logv2fs migrate --type=data --batch-size=500 --skip-existing
`,
	Run: func(cmd *cobra.Command, args []string) {
		// è·å–å‘½ä»¤è¡Œå‚æ•°
		migrationType, _ := cmd.Flags().GetString("type")
		batchSize, _ := cmd.Flags().GetInt("batch-size")
		skipExisting, _ := cmd.Flags().GetBool("skip-existing")

		log.Printf("ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®åº“è¿ç§»ï¼Œç±»å‹: %s", migrationType)

		// åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
		stats := &model.MigrationStats{
			StartTime: time.Now(),
			Errors:    []string{},
		}

		// æ‰§è¡Œè¿ç§»
		switch migrationType {
		case "schema":
			err := migrateSchema()
			if err != nil {
				log.Fatalf("âŒ æ¨¡å¼è¿ç§»å¤±è´¥: %v", err)
			}
			log.Println("âœ… æ¨¡å¼è¿ç§»å®Œæˆ")
		case "data":
			err := migrateData(batchSize, skipExisting, stats)
			if err != nil {
				log.Fatalf("âŒ æ•°æ®è¿ç§»å¤±è´¥: %v", err)
			}
		case "full":
			// å…ˆåˆ›å»ºæ¨¡å¼
			err := migrateSchema()
			if err != nil {
				log.Fatalf("âŒ æ¨¡å¼è¿ç§»å¤±è´¥: %v", err)
			}
			log.Println("âœ… æ¨¡å¼è¿ç§»å®Œæˆ")

			// å†è¿ç§»æ•°æ®
			err = migrateData(batchSize, skipExisting, stats)
			if err != nil {
				log.Fatalf("âŒ æ•°æ®è¿ç§»å¤±è´¥: %v", err)
			}
		default:
			log.Fatalf("âŒ ä¸æ”¯æŒçš„è¿ç§»ç±»å‹: %s", migrationType)
		}

		// è¾“å‡ºè¿ç§»ç»Ÿè®¡ä¿¡æ¯
		stats.EndTime = time.Now()
		printMigrationStats(stats)
	},
}

// migrateSchema åˆ›å»ºPostgreSQLè¡¨ç»“æ„
func migrateSchema() error {
	log.Println("ğŸ“‹ å¼€å§‹åˆ›å»ºPostgreSQLæ•°æ®åº“å’Œè¡¨ç»“æ„...")

	// åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
	err := database.CreateDatabaseIfNotExists()
	if err != nil {
		return fmt.Errorf("åˆ›å»ºæ•°æ®åº“å¤±è´¥: %v", err)
	}

	// åˆå§‹åŒ–PostgreSQLè¿æ¥
	db := database.InitPostgreSQL()

	// å¯ç”¨PostgreSQLæ‰©å±•
	err = enablePostgresExtensions(db)
	if err != nil {
		log.Printf("âš ï¸  å¯ç”¨PostgreSQLæ‰©å±•å¤±è´¥: %v", err)
	}

	// è‡ªåŠ¨è¿ç§»è¡¨ç»“æ„
	err = db.AutoMigrate(
		&model.NodeTrafficLogsPG{},
		&model.UserTrafficLogsPG{},
		&model.ExpiryCheckDomainInfoPG{},
		&model.SubscriptionNodePG{},
	)
	if err != nil {
		return fmt.Errorf("è‡ªåŠ¨è¿ç§»å¤±è´¥: %v", err)
	}

	// åˆ›å»ºå¿…è¦çš„ç´¢å¼•
	err = createCustomIndexes(db)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºç´¢å¼•å¤±è´¥: %v", err)
	}

	log.Println("âœ… PostgreSQLè¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
	return nil
}

// migrateData è¿ç§»æ•°æ®ä»MongoDBåˆ°PostgreSQL
func migrateData(batchSize int, skipExisting bool, stats *model.MigrationStats) error {
	log.Println("ğŸ“¦ å¼€å§‹æ•°æ®è¿ç§»...")

	// è·å–æ•°æ®åº“è¿æ¥
	mongoClient := database.Client
	postgresDB := database.GetPostgresDB()

	// éªŒè¯è¿æ¥
	if mongoClient == nil {
		return fmt.Errorf("MongoDBè¿æ¥æœªåˆå§‹åŒ–")
	}
	if postgresDB == nil {
		return fmt.Errorf("PostgreSQLè¿æ¥æœªåˆå§‹åŒ–")
	}

	// è¿ç§»ExpiryCheckDomains
	err := migrateExpiryCheckDomainsData(batchSize, skipExisting, stats)
	if err != nil {
		return fmt.Errorf("ExpiryCheckDomainsè¿ç§»å¤±è´¥: %v", err)
	}

	// è¿ç§»SubscriptionNodes
	err = migrateSubscriptionNodesData(batchSize, skipExisting, stats)
	if err != nil {
		return fmt.Errorf("SubscriptionNodesè¿ç§»å¤±è´¥: %v", err)
	}

	// è¿ç§»NodeTrafficLogs
	err = migrateNodeTrafficLogsData(batchSize, skipExisting, stats)
	if err != nil {
		return fmt.Errorf("NodeTrafficLogsè¿ç§»å¤±è´¥: %v", err)
	}

	// è¿ç§»UserTrafficLogs
	err = migrateUserTrafficLogsData(batchSize, skipExisting, stats)
	if err != nil {
		return fmt.Errorf("UserTrafficLogsè¿ç§»å¤±è´¥: %v", err)
	}

	log.Println("âœ… æ•°æ®è¿ç§»å®Œæˆ")
	return nil
}

// migrateExpiryCheckDomainsData è¿ç§»ExpiryCheckDomainsæ•°æ®
func migrateExpiryCheckDomainsData(batchSize int, skipExisting bool, stats *model.MigrationStats) error {
	log.Println("ğŸ”„ å¼€å§‹è¿ç§»ExpiryCheckDomainsæ•°æ®...")

	// è·å–æ•°æ®åº“è¿æ¥
	postgresDB := database.GetPostgresDB()

	// è·å–MongoDBé›†åˆ
	expiryCheckDomainCol := database.GetCollection(model.ExpiryCheckDomainInfo{})

	// æŸ¥è¯¢æ‰€æœ‰è®°å½•
	ctx := context.Background()
	cursor, err := expiryCheckDomainCol.Find(ctx, bson.M{})
	if err != nil {
		return fmt.Errorf("æŸ¥è¯¢MongoDB ExpiryCheckDomainså¤±è´¥: %v", err)
	}
	defer cursor.Close(ctx)

	// è¿ç§»æ•°æ®
	var migratedCount int64
	for cursor.Next(ctx) {
		// è§£æMongoDBè®°å½•
		var mongoDomain model.ExpiryCheckDomainInfo
		if err := cursor.Decode(&mongoDomain); err != nil {
			stats.Errors = append(stats.Errors, fmt.Sprintf("è§£æMongoDB ExpiryCheckDomainå¤±è´¥: %v", err))
			continue
		}

		// æ£€æŸ¥PostgreSQLä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥è®°å½•
		var existingCount int64
		if err := postgresDB.Model(&model.ExpiryCheckDomainInfoPG{}).Where("domain = ?", mongoDomain.Domain).Count(&existingCount).Error; err != nil {
			stats.Errors = append(stats.Errors, fmt.Sprintf("æ£€æŸ¥PostgreSQL ExpiryCheckDomainæ˜¯å¦å­˜åœ¨å¤±è´¥: %v", err))
			continue
		}

		if existingCount > 0 && skipExisting {
			log.Printf("è·³è¿‡å·²å­˜åœ¨çš„ExpiryCheckDomainè®°å½•: %s", mongoDomain.Domain)
			continue
		}

		// åˆ›å»ºPostgreSQLè®°å½•
		pgDomain := model.ExpiryCheckDomainInfoPG{
			ID:           uuid.New(),
			Domain:       mongoDomain.Domain,
			Remark:       mongoDomain.Remark,
			ExpiredDate:  mongoDomain.ExpiredDate,
			DaysToExpire: mongoDomain.DaysToExpire,
			CreatedAt:    time.Now(),
			UpdatedAt:    time.Now(),
		}

		// æ’å…¥æˆ–æ›´æ–°è®°å½•
		if existingCount > 0 {
			if err := postgresDB.Model(&model.ExpiryCheckDomainInfoPG{}).Where("domain = ?", mongoDomain.Domain).Updates(map[string]interface{}{
				"remark":         pgDomain.Remark,
				"expired_date":   pgDomain.ExpiredDate,
				"days_to_expire": pgDomain.DaysToExpire,
				"updated_at":     pgDomain.UpdatedAt,
			}).Error; err != nil {
				stats.Errors = append(stats.Errors, fmt.Sprintf("æ›´æ–°PostgreSQL ExpiryCheckDomainå¤±è´¥: %v", err))
				continue
			}
		} else {
			if err := postgresDB.Create(&pgDomain).Error; err != nil {
				stats.Errors = append(stats.Errors, fmt.Sprintf("æ’å…¥PostgreSQL ExpiryCheckDomainå¤±è´¥: %v", err))
				continue
			}
		}

		migratedCount++
		if migratedCount%int64(batchSize) == 0 {
			log.Printf("å·²è¿ç§» %d æ¡ExpiryCheckDomainè®°å½•", migratedCount)
		}
	}

	// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
	stats.DomainRecordsMigrated += migratedCount
	log.Printf("âœ… ExpiryCheckDomainsè¿ç§»å®Œæˆï¼Œå…±è¿ç§» %d æ¡è®°å½•", migratedCount)
	return nil
}

// migrateSubscriptionNodesData è¿ç§»SubscriptionNodesæ•°æ®
func migrateSubscriptionNodesData(batchSize int, skipExisting bool, stats *model.MigrationStats) error {
	log.Println("ğŸ”„ å¼€å§‹è¿ç§»SubscriptionNodesæ•°æ®...")

	// è·å–æ•°æ®åº“è¿æ¥
	postgresDB := database.GetPostgresDB()

	// è·å–MongoDBé›†åˆ
	subNodesCol := database.GetCollection(model.SubscriptionNode{})

	// æŸ¥è¯¢æ‰€æœ‰è®°å½•
	ctx := context.Background()
	cursor, err := subNodesCol.Find(ctx, bson.M{})
	if err != nil {
		return fmt.Errorf("æŸ¥è¯¢MongoDB SubscriptionNodeså¤±è´¥: %v", err)
	}
	defer cursor.Close(ctx)

	// è¿ç§»æ•°æ®
	var migratedCount int64
	for cursor.Next(ctx) {
		// è§£æMongoDBè®°å½•
		var mongoNode model.SubscriptionNode
		if err := cursor.Decode(&mongoNode); err != nil {
			stats.Errors = append(stats.Errors, fmt.Sprintf("è§£æMongoDB SubscriptionNodeå¤±è´¥: %v", err))
			continue
		}

		// æ£€æŸ¥PostgreSQLä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥è®°å½•
		var existingCount int64
		if err := postgresDB.Model(&model.SubscriptionNodePG{}).Where("remark = ?", mongoNode.Remark).Count(&existingCount).Error; err != nil {
			stats.Errors = append(stats.Errors, fmt.Sprintf("æ£€æŸ¥PostgreSQL SubscriptionNodeæ˜¯å¦å­˜åœ¨å¤±è´¥: %v", err))
			continue
		}

		if existingCount > 0 && skipExisting {
			log.Printf("è·³è¿‡å·²å­˜åœ¨çš„SubscriptionNodeè®°å½•: %s", mongoNode.Remark)
			continue
		}

		// åˆ›å»ºPostgreSQLè®°å½•
		pgNode := model.SubscriptionNodePG{
			ID:           uuid.New(),
			Type:         mongoNode.Type,
			Remark:       mongoNode.Remark,
			Domain:       mongoNode.Domain,
			IP:           mongoNode.IP,
			SNI:          mongoNode.SNI,
			UUID:         mongoNode.UUID,
			Path:         mongoNode.PATH,
			ServerPort:   mongoNode.SERVER_PORT,
			Password:     mongoNode.PASSWORD,
			PublicKey:    mongoNode.PUBLIC_KEY,
			ShortID:      mongoNode.SHORT_ID,
			EnableOpenai: mongoNode.EnableOpenai,
			CreatedAt:    time.Now(),
			UpdatedAt:    time.Now(),
		}

		// æ’å…¥æˆ–æ›´æ–°è®°å½•
		if existingCount > 0 {
			if err := postgresDB.Model(&model.SubscriptionNodePG{}).Where("remark = ?", mongoNode.Remark).Updates(map[string]interface{}{
				"type":          pgNode.Type,
				"domain":        pgNode.Domain,
				"ip":            pgNode.IP,
				"sni":           pgNode.SNI,
				"uuid":          pgNode.UUID,
				"path":          pgNode.Path,
				"server_port":   pgNode.ServerPort,
				"password":      pgNode.Password,
				"public_key":    pgNode.PublicKey,
				"short_id":      pgNode.ShortID,
				"enable_openai": pgNode.EnableOpenai,
				"updated_at":    pgNode.UpdatedAt,
			}).Error; err != nil {
				stats.Errors = append(stats.Errors, fmt.Sprintf("æ›´æ–°PostgreSQL SubscriptionNodeå¤±è´¥: %v", err))
				continue
			}
		} else {
			if err := postgresDB.Create(&pgNode).Error; err != nil {
				stats.Errors = append(stats.Errors, fmt.Sprintf("æ’å…¥PostgreSQL SubscriptionNodeå¤±è´¥: %v", err))
				continue
			}
		}

		migratedCount++
		if migratedCount%int64(batchSize) == 0 {
			log.Printf("å·²è¿ç§» %d æ¡SubscriptionNodeè®°å½•", migratedCount)
		}
	}

	// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
	stats.SubscriptionNodesMigrated += migratedCount
	log.Printf("âœ… SubscriptionNodesè¿ç§»å®Œæˆï¼Œå…±è¿ç§» %d æ¡è®°å½•", migratedCount)
	return nil
}

// migrateNodeTrafficLogsData è¿ç§»NodeTrafficLogsæ•°æ®
func migrateNodeTrafficLogsData(batchSize int, skipExisting bool, stats *model.MigrationStats) error {
	return migrateNodeTrafficLogsDataImpl(batchSize, skipExisting, stats)
}

// migrateUserTrafficLogsData è¿ç§»UserTrafficLogsæ•°æ®
func migrateUserTrafficLogsData(batchSize int, skipExisting bool, stats *model.MigrationStats) error {
	return migrateUserTrafficLogsDataImpl(batchSize, skipExisting, stats)
}

// enablePostgresExtensions å¯ç”¨å¿…è¦çš„PostgreSQLæ‰©å±•
func enablePostgresExtensions(db *gorm.DB) error {
	log.Println("ğŸ”§ å¯ç”¨PostgreSQLæ‰©å±•...")

	extensions := []string{
		"CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\"", // UUIDç”Ÿæˆå‡½æ•°
		"CREATE EXTENSION IF NOT EXISTS \"pgcrypto\"",  // åŠ å¯†å‡½æ•°
	}

	for _, ext := range extensions {
		if err := db.Exec(ext).Error; err != nil {
			log.Printf("âš ï¸  å¯ç”¨æ‰©å±•å¤±è´¥: %s, é”™è¯¯: %v", ext, err)
			// ç»§ç»­æ‰§è¡Œï¼ŒæŸäº›æ‰©å±•å¯èƒ½å·²ç»å­˜åœ¨æˆ–è€…ä¸æ˜¯å¿…éœ€çš„
		}
	}

	log.Println("âœ… PostgreSQLæ‰©å±•å¯ç”¨å®Œæˆ")
	return nil
}

// createCustomIndexes åˆ›å»ºè‡ªå®šä¹‰ç´¢å¼•
func createCustomIndexes(db *gorm.DB) error {
	log.Println("ğŸ” åˆ›å»ºè‡ªå®šä¹‰ç´¢å¼•...")

	// åˆ›å»ºExpiryCheckDomainsè¡¨çš„ç´¢å¼•
	err := createExpiryCheckDomainsIndexes(db)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºExpiryCheckDomainsç´¢å¼•å¤±è´¥: %v", err)
	}

	// åˆ›å»ºNodeTrafficLogsè¡¨çš„ç´¢å¼•
	err = createNodeTrafficLogsIndexes(db)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºNodeTrafficLogsç´¢å¼•å¤±è´¥: %v", err)
	}

	// åˆ›å»ºUserTrafficLogsè¡¨çš„ç´¢å¼•
	err = createUserTrafficLogsIndexes(db)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºUserTrafficLogsç´¢å¼•å¤±è´¥: %v", err)
	}

	// åˆ›å»ºJSONBå­—æ®µçš„GINç´¢å¼•
	err = createJSONBIndexes(db)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºJSONBç´¢å¼•å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæ—¶é—´èŒƒå›´æŸ¥è¯¢ç´¢å¼•
	err = createTimeIndexes(db)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºæ—¶é—´ç´¢å¼•å¤±è´¥: %v", err)
	}

	log.Println("âœ… è‡ªå®šä¹‰ç´¢å¼•åˆ›å»ºå®Œæˆ")
	return nil
}

// createJSONBIndexes ä¸ºJSONBå­—æ®µåˆ›å»ºGINç´¢å¼•
func createJSONBIndexes(db *gorm.DB) error {
	log.Println("ğŸ“„ ä¸ºJSONBå­—æ®µåˆ›å»ºGINç´¢å¼•...")

	jsonbIndexes := []string{
		// UserTrafficLogsçš„JSONBç´¢å¼•
		"CREATE INDEX IF NOT EXISTS idx_user_traffic_logs_daily_logs ON user_traffic_logs USING GIN (daily_logs)",
		"CREATE INDEX IF NOT EXISTS idx_user_traffic_logs_monthly_logs ON user_traffic_logs USING GIN (monthly_logs)",
		"CREATE INDEX IF NOT EXISTS idx_user_traffic_logs_yearly_logs ON user_traffic_logs USING GIN (yearly_logs)",

		// NodeTrafficLogsçš„JSONBç´¢å¼•
		"CREATE INDEX IF NOT EXISTS idx_node_traffic_logs_daily_logs ON node_traffic_logs USING GIN (daily_logs)",
		"CREATE INDEX IF NOT EXISTS idx_node_traffic_logs_monthly_logs ON node_traffic_logs USING GIN (monthly_logs)",
		"CREATE INDEX IF NOT EXISTS idx_node_traffic_logs_yearly_logs ON node_traffic_logs USING GIN (yearly_logs)",
	}

	for _, indexSQL := range jsonbIndexes {
		if err := db.Exec(indexSQL).Error; err != nil {
			log.Printf("âš ï¸  åˆ›å»ºJSONBç´¢å¼•å¤±è´¥: %s, é”™è¯¯: %v", indexSQL, err)
		}
	}

	log.Println("âœ… JSONBç´¢å¼•åˆ›å»ºå®Œæˆ")
	return nil
}

// createExpiryCheckDomainsIndexes ä¸ºExpiryCheckDomainsè¡¨åˆ›å»ºç´¢å¼•
func createExpiryCheckDomainsIndexes(db *gorm.DB) error {
	log.Println("ğŸ” åˆ›å»ºExpiryCheckDomainsç´¢å¼•...")

	expiryCheckDomainsIndexes := []string{
		// åˆ›å»ºæ—¶é—´ç´¢å¼•
		"CREATE INDEX IF NOT EXISTS idx_expiry_check_domains_created_at ON expiry_check_domains(created_at)",
		// æ›´æ–°æ—¶é—´ç´¢å¼•
		"CREATE INDEX IF NOT EXISTS idx_expiry_check_domains_updated_at ON expiry_check_domains(updated_at)",
	}

	for _, indexSQL := range expiryCheckDomainsIndexes {
		if err := db.Exec(indexSQL).Error; err != nil {
			log.Printf("âš ï¸  åˆ›å»ºExpiryCheckDomainsç´¢å¼•å¤±è´¥: %s, é”™è¯¯: %v", indexSQL, err)
		}
	}

	log.Println("âœ… ExpiryCheckDomainsç´¢å¼•åˆ›å»ºå®Œæˆ")
	return nil
}

// createTimeIndexes åˆ›å»ºæ—¶é—´ç›¸å…³ç´¢å¼•
func createTimeIndexes(db *gorm.DB) error {
	log.Println("â° åˆ›å»ºæ—¶é—´ç›¸å…³ç´¢å¼•...")

	timeIndexes := []string{
		// åˆ›å»ºæ—¶é—´ç´¢å¼•
		"CREATE INDEX IF NOT EXISTS idx_user_traffic_logs_created_at ON user_traffic_logs(created_at)",
		"CREATE INDEX IF NOT EXISTS idx_node_traffic_logs_created_at ON node_traffic_logs(created_at)",

		// æ›´æ–°æ—¶é—´ç´¢å¼•
		"CREATE INDEX IF NOT EXISTS idx_user_traffic_logs_updated_at ON user_traffic_logs(updated_at)",
		"CREATE INDEX IF NOT EXISTS idx_node_traffic_logs_updated_at ON node_traffic_logs(updated_at)",
	}

	for _, indexSQL := range timeIndexes {
		if err := db.Exec(indexSQL).Error; err != nil {
			log.Printf("âš ï¸  åˆ›å»ºæ—¶é—´ç´¢å¼•å¤±è´¥: %s, é”™è¯¯: %v", indexSQL, err)
		}
	}

	log.Println("âœ… æ—¶é—´ç´¢å¼•åˆ›å»ºå®Œæˆ")
	return nil
}

// printMigrationStats æ‰“å°è¿ç§»ç»Ÿè®¡ä¿¡æ¯
func printMigrationStats(stats *model.MigrationStats) {
	duration := stats.EndTime.Sub(stats.StartTime)

	fmt.Println("\n" + strings.Repeat("=", 60))
	fmt.Println("ğŸ“Š æ•°æ®è¿ç§»ç»Ÿè®¡æŠ¥å‘Š")
	fmt.Println(strings.Repeat("=", 60))
	fmt.Printf("â±ï¸  è¿ç§»è€—æ—¶: %v\n", duration)
	fmt.Printf("ğŸ”— åŸŸåè®°å½•: %d\n", stats.DomainRecordsMigrated)
	fmt.Printf("ğŸŒ èŠ‚ç‚¹è®°å½•: %d\n", stats.NodeRecordsMigrated)
	fmt.Printf("ğŸ‘¥ ç”¨æˆ·è®°å½•: %d\n", stats.UserRecordsMigrated)
	fmt.Printf("ğŸ“¡ è®¢é˜…èŠ‚ç‚¹: %d\n", stats.SubscriptionNodesMigrated)

	totalRecords := stats.DomainRecordsMigrated + stats.NodeRecordsMigrated + stats.UserRecordsMigrated + stats.SubscriptionNodesMigrated
	fmt.Printf("ğŸ“ˆ æ€»è®°å½•æ•°: %d\n", totalRecords)

	if duration.Seconds() > 0 {
		rate := float64(totalRecords) / duration.Seconds()
		fmt.Printf("âš¡ è¿ç§»é€Ÿç‡: %.2f è®°å½•/ç§’\n", rate)
	}

	if len(stats.Errors) > 0 {
		fmt.Printf("âŒ é”™è¯¯æ•°é‡: %d\n", len(stats.Errors))
		fmt.Println("\né”™è¯¯è¯¦æƒ…:")
		for i, err := range stats.Errors {
			if i < 10 { // åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
				fmt.Printf("  %d. %s\n", i+1, err)
			}
		}
		if len(stats.Errors) > 10 {
			fmt.Printf("  ... è¿˜æœ‰ %d ä¸ªé”™è¯¯æœªæ˜¾ç¤º\n", len(stats.Errors)-10)
		}
	} else {
		fmt.Println("âœ… è¿ç§»è¿‡ç¨‹æ— é”™è¯¯")
	}

	fmt.Println(strings.Repeat("=", 60))
	fmt.Println("ğŸ‰ æ•°æ®åº“è¿ç§»å®Œæˆï¼")
	fmt.Println("ğŸ’¡ å»ºè®®: è¿ç§»å®Œæˆåè¯·éªŒè¯æ•°æ®å®Œæ•´æ€§å¹¶è¿›è¡Œæ€§èƒ½æµ‹è¯•")
	fmt.Println(strings.Repeat("=", 60))
}

func init() {
	rootCmd.AddCommand(migrateCmd)

	// æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
	migrateCmd.Flags().StringP("type", "t", "full", "è¿ç§»ç±»å‹: schema(ä»…ç»“æ„), data(ä»…æ•°æ®), full(å®Œæ•´è¿ç§»)")
	migrateCmd.Flags().IntP("batch-size", "b", 1000, "æ‰¹å¤„ç†å¤§å° (æ¨è: 500-2000)")
	migrateCmd.Flags().BoolP("skip-existing", "s", false, "è·³è¿‡å·²å­˜åœ¨çš„è®°å½•")
}
